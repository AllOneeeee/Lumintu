{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "# from tf2crf import CRF\n",
    "\n",
    "import gc\n",
    "\n",
    "# from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dropout, Dense, TimeDistributed, MaxPool1D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,TensorBoard,EarlyStopping\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# from tensorflow.keras.layers import Dense, Flatten, Embedding, Input, LSTM, Conv1D, MaxPool1D, Bidirectional, Dropout, BatchNormalization\n",
    "\n",
    "\n",
    "# from skmultilearn.problem_transform import LabelPowerset\n",
    "# from sklearn.multioutput import ClassifierChain\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, hamming_loss, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from keras import layers\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "from keras_preprocessing import sequence\n",
    "from keras_preprocessing.text import Tokenizer\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "\n",
    "import seaborn as sns\n",
    "# import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# from skmultilearn.problem_transform import BinaryRelevance\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED_MODEL_NAME = 'indobenchmark/indobert-base-p1'\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset size: 3194\n",
      "valset size: 798\n",
      "testset size:  998\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class FakeNewsDataset(Dataset):\n",
    "    def __init__(self, mode, tokenizer):\n",
    "        assert mode in ['train', 'val', 'test']\n",
    "        self.mode = mode\n",
    "        self.df = pd.read_csv(mode + '.tsv', sep='\\t').fillna(\"\")\n",
    "        self.len = len(self.df)\n",
    "        self.tokenizer = tokenizer  # BERT tokenizer\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode == 'test':\n",
    "            statement, label = self.df.iloc[idx, :].values\n",
    "            label_tensor = torch.tensor(label)\n",
    "        else:\n",
    "            statement, label = self.df.iloc[idx, :].values\n",
    "            label_tensor = torch.tensor(label)\n",
    "            \n",
    "        word_pieces = ['[CLS]']\n",
    "        statement = self.tokenizer.tokenize(statement)\n",
    "        word_pieces += statement + ['[SEP]']\n",
    "        len_st = len(word_pieces)\n",
    "        \n",
    "        ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
    "        tokens_tensor = torch.tensor(ids)\n",
    "        \n",
    "        segments_tensor = torch.tensor([0] * len_st, dtype=torch.long)\n",
    "        \n",
    "        return (tokens_tensor, segments_tensor, label_tensor)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    \n",
    "# Initialize Datasets for Transformation\n",
    "trainset = FakeNewsDataset('train', tokenizer=tokenizer)\n",
    "valset = FakeNewsDataset('val', tokenizer=tokenizer)\n",
    "testset = FakeNewsDataset('test', tokenizer=tokenizer)\n",
    "\n",
    "print('trainset size:' ,trainset.__len__())\n",
    "print('valset size:',valset.__len__())\n",
    "print('testset size: ',testset.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "original_statement: \n",
      "anies ganjar kemarin ngasih kritik gibran sopan mahfud bain dukung\n",
      "\n",
      "tokens: \n",
      "['[CLS]', 'anies', 'ganja', '##r', 'kemarin', 'ngasih', 'kritik', 'gib', '##ran', 'sopan', 'mahfud', 'bai', '##n', 'dukung', '[SEP]']\n",
      "\n",
      "label: 5\n",
      "\n",
      "--------------------\n",
      "\n",
      "tokens_tensor: \n",
      "tensor([    2, 19145, 18006, 30359,  3601, 13710,  6476, 19044,  3823,  9036,\n",
      "        20469, 11217, 30355,  9162,     3])\n",
      "\n",
      "segments_tensor: \n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "label_tensor: \n",
      "5\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_idx = 0\n",
    "statement, label = trainset.df.iloc[sample_idx].values\n",
    "tokens_tensor, segments_tensor, label_tensor = trainset[sample_idx]\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokens_tensor.tolist())\n",
    "combined_text = \" \".join(tokens)\n",
    "\n",
    "print(f\"\"\"\n",
    "original_statement: \n",
    "{statement}\n",
    "\n",
    "tokens: \n",
    "{tokens}\n",
    "\n",
    "label: {label}\n",
    "\n",
    "--------------------\n",
    "\n",
    "tokens_tensor: \n",
    "{tokens_tensor}\n",
    "\n",
    "segments_tensor: \n",
    "{segments_tensor}\n",
    "\n",
    "label_tensor: \n",
    "{label_tensor}\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def create_mini_batch(samples):\n",
    "    tokens_tensors = [s[0] for s in samples]\n",
    "    segments_tensors = [s[1] for s in samples]\n",
    "    \n",
    "    if samples[0][2] is not None:\n",
    "        label_ids = torch.stack([s[2] for s in samples])\n",
    "    else:\n",
    "        label_ids = None\n",
    "    \n",
    "    tokens_tensors = pad_sequence(tokens_tensors, batch_first=True)\n",
    "    segments_tensors = pad_sequence(segments_tensors, batch_first=True)\n",
    "    \n",
    "\n",
    "    masks_tensors = torch.zeros(tokens_tensors.shape, dtype=torch.long)\n",
    "    masks_tensors = masks_tensors.masked_fill(tokens_tensors != 0, 1)\n",
    "    \n",
    "    return tokens_tensors, segments_tensors, masks_tensors, label_ids\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, collate_fn=create_mini_batch)\n",
    "valloader = DataLoader(valset, batch_size=BATCH_SIZE, collate_fn=create_mini_batch)\n",
    "testloader = DataLoader(testset, batch_size=BATCH_SIZE,collate_fn=create_mini_batch)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tokens_tensors.shape   = torch.Size([16, 23]) \n",
      "tensor([[    2, 19145, 18006, 30359,  3601, 13710,  6476, 19044,  3823,  9036,\n",
      "         20469, 11217, 30355,  9162,     3,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [    2, 19145,  1023,  8137,  9420, 21167,  5822, 10634,  3500,     3,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [    2, 19145,  2911, 14979,  2424,  1375,  3341, 27715,  5454,  5035,\n",
      "          8292, 22710,   678,  9558, 12058,     3,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [    2,  3681,    84,  3681,    84,  8181,  3624,  2655,   986,   986,\n",
      "          4772,  1587,  3577,   986,  2501, 15994, 18006, 30359,  7979,  6378,\n",
      "         20469,     3,     0],\n",
      "        [    2, 11283,  2873, 11935,  2690,   494, 23918,     3,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [    2,   232, 13744,   737,  5234,  3686,  6359, 12056,  4299,  4938,\n",
      "          6919, 19145,  4442,    16,  5234,  3686,  6359,  3256,     3,     0,\n",
      "             0,     0,     0],\n",
      "        [    2,   368,    55,  9148, 11283, 19044,  3823,  1437,  5694,  7343,\n",
      "             3,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [    2, 11119,   300,  9162,  8990, 18977,  4395,    25, 11326,  2376,\n",
      "          7237, 11119,   300,   959, 19145,  2078,  3714,    17,  1871,  3022,\n",
      "          1871, 26249,     3],\n",
      "        [    2,  1554, 10736, 11971,  2501, 15994, 25951, 18006, 30359, 20469,\n",
      "          3797,  2392,  1081,  3921, 18006, 30359, 20469,  4364, 16729,   300,\n",
      "             3,     0,     0],\n",
      "        [    2, 15994, 18006, 30359,  7979,  6378, 25951, 20469,  4076,  6767,\n",
      "            17, 11409,   885,  6231, 12721,  1166,     3,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [    2,  1410, 11119,   300, 23834,  1166,  7633,    72,   552,     3,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [    2,  2670, 26289,   282,  9162,  2501, 18006, 30359,  7979,  6378,\n",
      "         20469, 16729,  1642, 15140,     3,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [    2, 12188,  4626,  6378,  6448,  3823, 13496,  1447,  4945, 30378,\n",
      "          5651,   342,   552, 23677,  1642,     3,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [    2, 11283,  5381,  2692,  4680, 10730,  2775, 14949,  5774,    15,\n",
      "          1661,  3164, 10219, 12634,  8851,  1829,     3,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [    2,  7364,  7455,  1172,   717, 22931, 30359,  2690,  3423,  4879,\n",
      "         14979,  3601,     3,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [    2,  1699,  8508,  6169, 10081,  1224,   421,  6159,  6613,  8508,\n",
      "          3924, 19145,  5134,  2869,    55, 10640,     3,     0,     0,     0,\n",
      "             0,     0,     0]])\n",
      "------------------------\n",
      "segments_tensors.shape = torch.Size([16, 23])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "------------------------\n",
      "masks_tensors.shape    = torch.Size([16, 23])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]])\n",
      "------------------------\n",
      "label_ids.shape        = torch.Size([16])\n",
      "tensor([5, 3, 6, 5, 5, 5, 5, 7, 5, 5, 6, 3, 1, 6, 4, 0])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = next(iter(trainloader))\n",
    "\n",
    "tokens_tensors, segments_tensors, masks_tensors, label_ids = data\n",
    "\n",
    "print(f\"\"\"\n",
    "tokens_tensors.shape   = {tokens_tensors.shape} \n",
    "{tokens_tensors}\n",
    "------------------------\n",
    "segments_tensors.shape = {segments_tensors.shape}\n",
    "{segments_tensors}\n",
    "------------------------\n",
    "masks_tensors.shape    = {masks_tensors.shape}\n",
    "{masks_tensors}\n",
    "------------------------\n",
    "label_ids.shape        = {label_ids.shape}\n",
    "{label_ids}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "name             module\n",
      "-----------------------\n",
      "bert:embeddings\n",
      "bert:encoder\n",
      "bert:pooler\n",
      "dropout          Dropout(p=0.1, inplace=False)\n",
      "classifier       Linear(in_features=768, out_features=8, bias=True)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "NUM_LABELS = 8\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    PRETRAINED_MODEL_NAME, num_labels=NUM_LABELS)\n",
    "\n",
    "clear_output()\n",
    "\n",
    "print(\"\"\"\n",
    "name             module\n",
    "-----------------------\"\"\")\n",
    "for name, module in model.named_children():\n",
    "    if name == \"bert\":\n",
    "        for n, _ in module.named_children():\n",
    "            print(f\"{name}:{n}\")\n",
    "    else:\n",
    "        print(\"{:16} {}\".format(name, module))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT_BiLSTM(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(50000, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (lstm): LSTM(768, 100, batch_first=True, bidirectional=True)\n",
      "  (classifier): Linear(in_features=200, out_features=8, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class BERT_BiLSTM(nn.Module):\n",
    "    def __init__(self, pretrained_model_name, hidden_dim, num_labels):\n",
    "        super(BERT_BiLSTM, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(pretrained_model_name)\n",
    "        self.lstm = nn.LSTM(input_size=self.bert.config.hidden_size,\n",
    "                            hidden_size=hidden_dim,\n",
    "                            num_layers=1,\n",
    "                            bidirectional=True,\n",
    "                            batch_first=True)\n",
    "        self.classifier = nn.Linear(hidden_dim * 2, num_labels)\n",
    "    \n",
    "    def forward(self, input_ids, token_type_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, \n",
    "                            token_type_ids=token_type_ids, \n",
    "                            attention_mask=attention_mask)\n",
    "        \n",
    "        sequence_output = outputs[0]  # shape: (batch_size, sequence_length, hidden_size)\n",
    "        \n",
    "        lstm_output, _ = self.lstm(sequence_output)  # shape: (batch_size, sequence_length, hidden_dim*2)\n",
    "        \n",
    "        lstm_output = lstm_output[:, 0, :]  # Take the output of the first token (CLS token)\n",
    "        \n",
    "        logits = self.classifier(lstm_output)  # shape: (batch_size, num_labels)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Initialize the model\n",
    "NUM_LABELS = 8\n",
    "HIDDEN_DIM = 100\n",
    "model = BERT_BiLSTM(PRETRAINED_MODEL_NAME, HIDDEN_DIM, NUM_LABELS)\n",
    "\n",
    "# Print the model architecture\n",
    "print(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example training loop (simplified)\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=2e-5)\n",
    "loss_fn = CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(3):  # Example: 3 epochs\n",
    "    model.train()\n",
    "    for data in trainloader:\n",
    "        tokens_tensors, segments_tensors, masks_tensors, label_ids = [t.to(device) for t in data]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(tokens_tensors, segments_tensors, masks_tensors)\n",
    "        loss = loss_fn(outputs, label_ids)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
    "    \n",
    "# Validation loop (simplified)\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in valloader:\n",
    "        tokens_tensors, segments_tensors, masks_tensors, label_ids = [t.to(device) for t in data]\n",
    "        \n",
    "        outputs = model(tokens_tensors, segments_tensors, masks_tensors)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        total += label_ids.size(0)\n",
    "        correct += (predicted == label_ids).sum().item()\n",
    "\n",
    "print(f\"Validation Accuracy: {100 * correct / total}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
